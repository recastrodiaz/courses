{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.trainer as trainer\n",
    "import torch.utils.trainer.plugins\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from imagedataset import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# train_dir = \"train\"\n",
    "train_dir = \"sample\"\n",
    "use_cuda = False\n",
    "batch_size = 2\n",
    "print('Using CUDA:', use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"data/galaxies/\"\n",
    "\n",
    "def dataframe_from_csv():\n",
    "    df=pd.read_csv(data_path + \"classes.csv\", sep=',')\n",
    "    df.set_index(\"GalaxyID\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in train folder: 10\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import imagedataset\n",
    "importlib.reload(imagedataset)\n",
    "\n",
    "# Data loading code\n",
    "traindir = os.path.join(data_path, train_dir )\n",
    "valdir = os.path.join(data_path, 'valid') \n",
    "testdir = os.path.join(data_path, 'test')\n",
    "\n",
    "targets = dataframe_from_csv();\n",
    "\n",
    "# pytorch way of implementing fastai's get_batches, (utils.py)\n",
    "def get_data_loader(dirname, shuffle=True, batch_size = 64):\n",
    "    image_dataset = ImageDataset(dirname, targets)\n",
    "    return torch.utils.data.DataLoader(image_dataset, batch_size=batch_size, \n",
    "                                       shuffle=shuffle, pin_memory=use_cuda), image_dataset\n",
    "\n",
    "train_loader, train_dataset = get_data_loader(traindir, batch_size=batch_size)\n",
    "\n",
    "print('Images in train folder:', len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 37 classes: Index(['Class1.1', 'Class1.2', 'Class1.3', 'Class2.1', 'Class2.2', 'Class3.1',\n",
      "       'Class3.2', 'Class4.1', 'Class4.2', 'Class5.1', 'Class5.2', 'Class5.3',\n",
      "       'Class5.4', 'Class6.1', 'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3',\n",
      "       'Class8.1', 'Class8.2', 'Class8.3', 'Class8.4', 'Class8.5', 'Class8.6',\n",
      "       'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', 'Class10.1',\n",
      "       'Class10.2', 'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3',\n",
      "       'Class11.4', 'Class11.5', 'Class11.6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Finetune by replacing the last fully connected layer and freezing all network parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the last fully-connected layer matching the new class count\n",
    "num_classes = len(targets.columns)\n",
    "print('Using {:d} classes: {}'.format(num_classes, targets.columns))\n",
    "expansion = 4 # TODO use Bottleneck.expansion instead\n",
    "model.fc = nn.Sequential(nn.Linear(512 * expansion, num_classes), nn.Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# enable cuda if available\n",
    "if(use_cuda):\n",
    "    model.cuda()\n",
    "    criterion.cuda()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainer():\n",
    "    # fine-tune with new classes\n",
    "    t = trainer.Trainer(model, criterion, optimizer, train_loader)\n",
    "    t.register_plugin(trainer.plugins.ProgressMonitor())\n",
    "    t.register_plugin(trainer.plugins.LossMonitor())\n",
    "    t.register_plugin(trainer.plugins.TimeMonitor())\n",
    "    t.register_plugin(trainer.plugins.Logger(['progress', 'loss', 'time']))\n",
    "    \n",
    "    # Requires a monkey patched version of trainery.py that calls\n",
    "    # input_var = Variable(batch_input.cuda())    # Line 57\n",
    "    # target_var = Variable(batch_target.cuda())  # Line 58\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 1/5 (20.00%)\tloss: 0.0577  (0.0173)\ttime: 0ms  (0ms)\n",
      "progress: 2/5 (40.00%)\tloss: 0.0905  (0.0393)\ttime: 606ms  (182ms)\n",
      "progress: 3/5 (60.00%)\tloss: 0.0666  (0.0475)\ttime: 598ms  (307ms)\n",
      "progress: 4/5 (80.00%)\tloss: 0.0755  (0.0559)\ttime: 586ms  (390ms)\n",
      "progress: 5/5 (100.00%)\tloss: 0.0614  (0.0575)\ttime: 584ms  (449ms)\n",
      "################################################################################\n",
      "Epoch summary:\n",
      "loss: 0.0703\ttime: 475ms\n",
      "################################################################################\n"
     ]
    }
   ],
   "source": [
    "t = getTrainer()\n",
    "epochs = 1\n",
    "model.train()\n",
    "t.run(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "val_loader, val_dataset = get_data_loader(valdir, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def get_error(val_loader):\n",
    "    # Process each mini-batch and accumulate all correct classifications\n",
    "    num_batches = sum(1 for b in enumerate(val_loader))\n",
    "    batches = enumerate(val_loader)\n",
    "    error2 = 0\n",
    "    for i, (images, labels) in batches:\n",
    "        sys.stdout.write('\\rBatch: {:d}/{:d}'.format(i + 1, num_batches))\n",
    "        sys.stdout.flush()\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "        predictions = model(Variable(images))\n",
    "        error2 += labels.sub(predictions.data).pow(2).sum()\n",
    "    # Avoid carriage return\n",
    "    print('')\n",
    "    return np.sqrt(error / len(val_loader.dataset.images_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print('RMSE for validation set: {}'.format(get_error(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_loader, test_dataset = get_data_loader(testdir, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def predict(loader):\n",
    "    # Process each mini-batch and accumulate all correct classifications\n",
    "    all_predictions_df = pd.DataFrame(data=None, columns=targets.columns,index=targets.index)\n",
    "    # Drop all rows\n",
    "    all_predictions_df.drop(targets.index, inplace=True)\n",
    "    num_batches = sum(1 for b in enumerate(loader))\n",
    "    batches = enumerate(loader)\n",
    "    current_image = 0\n",
    "    for i, (images, labels) in batches:\n",
    "        sys.stdout.write('\\rBatch: {:d}/{:d}'.format(i + 1, num_batches))\n",
    "        sys.stdout.flush()\n",
    "        if use_cuda:\n",
    "            images = images.cuda()\n",
    "        predictions = model(Variable(images))\n",
    "        batch_predictions = predictions.data.cpu().numpy()\n",
    "        for row_prediction in batch_predictions:\n",
    "            image_id = loader.dataset.images_idx_to_id[current_image]\n",
    "            all_predictions_df.loc[image_id] = row_prediction\n",
    "            current_image += 1\n",
    "    # Avoid carriage return\n",
    "    print('')\n",
    "    return all_predictions_df\n",
    "\n",
    "def save_kaggle_predictions(loader):\n",
    "    all_predictions_df = predict(loader)\n",
    "    all_predictions_df.to_csv(data_path + \"predicted-resnet.csv\")\n",
    "    return all_predictions_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5/5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='data/galaxies/predicted-resnet.csv' target='_blank'>data/galaxies/predicted-resnet.csv</a><br>"
      ],
      "text/plain": [
       "/Users/rodrigo/Libs/fastai-courses/deeplearning1/nbs/data/galaxies/predicted-resnet.csv"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "model.eval()\n",
    "save_kaggle_predictions(train_loader)\n",
    "FileLink(data_path + \"predicted-resnet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
